{
  
    
        "post0": {
            "title": "High frequencies and low amplitude SSVEP",
            "content": "High frequencies and low amplitude SSVEP-based BCI . We believe that one of the major hurdles for disseminating BCI technology from laboratories to use in the wild by a general audience is the poor user experience. For instance, Steady-States Visually Evoked Potentials (SSVEP) refer to the sustained rhythmic activity observed in surface electroencephalography (EEG) in response to the presentation of Repetitive Visual Stimuli (RVS). Due to their robustness and rapid onset, SSVEP has been widely used in Brain Computer Interfaces (BCI). However, typical SSVEP stimuli are straining to the eyes and present risks of triggering epileptic seizures. Reducing visual stimuli contrast or extending their frequency range both appear as relevant solutions to address these issues. It however remains sparsely documented how BCI performance is impacted by these features and to which extent user experience can be improved. In that respect, we have focused on the improvement of user comfort by making SSVEP stimuli less intrusive. . Reduction of stimuli depth . Our first results on SSVEP-BCI with reduction of stimuli depth can be found here. Using two popular decoding algorithms, Canonical Correlation Analysis (CCA) and Task-Related Component Analysis (TRCA) we have demonstrated that a SSVEP-BCI is still operable with only 10% of amplitude depth. This large amplitude reduction provided a significant improvement in the subjective rating of comfort by our users. . High frequencies (&gt;30Hz) SSVEP-BCI . We have then investigated the use of high frequencies and a combination with amplitude reduction. This work is currently under review and will be published soon. The results revealed that although high-frequency stimuli improve visual comfort, their classification performance was not competitive enough to design a reliable/responsive BCI. Importantly, we found that the amplitude depth reduction of low-frequency RVS is an effective solution to improve the user experience while maintaining high classification performance. These findings were further validated by an online T9 SSVEP-BCI in which stimuli with 40% amplitude depth reduction achieved comparable results (&gt;90% accuracy) to full amplitude stimuli while significantly improving user experience. .",
            "url": "https://ludovicdmt.github.io/ludodarmet/research/ssvep/2022/01/01/SSVEP-comfort.html",
            "relUrl": "/research/ssvep/2022/01/01/SSVEP-comfort.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Classification de feuilles d'arbres",
            "content": "Crédits : https://www.kaggle.com/abhmul/keras-convnet-lb-0-0052-w-visualization . Import des packages que l&#39;on va utiliser . %matplotlib inline import os import numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import time import pickle import cv2 import warnings from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from sklearn.model_selection import StratifiedShuffleSplit from sklearn.linear_model import LogisticRegressionCV, LogisticRegression from sklearn.metrics import accuracy_score from tqdm import tqdm_notebook # Keras stuff from keras.utils.np_utils import to_categorical from keras.preprocessing.image import img_to_array, load_img from keras.callbacks import TensorBoard . import keras print(&#39;The keras version is {}.&#39;.format(keras.__version__)) import tensorflow as tf print(&#39;The tensorflow version is {}.&#39;.format(tf.__version__)) import sklearn print(&#39;The scikit-learn version is {}.&#39;.format(sklearn.__version__)) . The keras version is 2.7.0. The tensorflow version is 2.7.0. The scikit-learn version is 1.0. . On fixe certains param&#232;tres et l&#39;al&#233;atoire . root = os.getcwd() np.random.seed(25) split_random_state = 5 split = .85 . Chargement des donn&#233;es . def cross_entropy(predictions, targets, epsilon=1e-12): &quot;&quot;&quot; Computes cross entropy between targets (encoded as one-hot vectors) and predictions. Input: predictions (N, k) ndarray targets (N, k) ndarray Returns: scalar &quot;&quot;&quot; predictions = np.clip(predictions, epsilon, 1. - epsilon) N = predictions.shape[0] ce = -np.sum(targets*np.log(predictions+1e-9))/N return ce . def load_numeric_training(standardize=True): &quot;&quot;&quot; Loads the pre-extracted features for the training data and returns a tuple of the image ids, the data, and the labels &quot;&quot;&quot; # Read data from the CSV file data = pd.read_csv(os.path.join(root, &#39;data/train.csv&#39;)) ID = data.pop(&#39;id&#39;) # Since the labels are textual, so we encode them categorically y = data.pop(&#39;species&#39;) y = LabelEncoder().fit(y).transform(y) # standardize the data by setting the mean to 0 and std to 1 X = StandardScaler().fit(data).transform(data) if standardize else data.values return ID, X, y def load_numeric_test(standardize=True): &quot;&quot;&quot; Loads the pre-extracted features for the test data and returns a tuple of the image ids, the data &quot;&quot;&quot; test = pd.read_csv(os.path.join(root, &#39;data/test.csv&#39;)) ID = test.pop(&#39;id&#39;) # standardize the data by setting the mean to 0 and std to 1 test = StandardScaler().fit(test).transform(test) if standardize else test.values return ID, test def resize_img(img, max_dim=128): &quot;&quot;&quot; Resize the image to so the maximum side is of size max_dim Returns a new image of the right size &quot;&quot;&quot; # Get the axis with the larger dimension max_ax = max((0, 1), key=lambda i: img.size[i]) # Scale both axes so the image&#39;s largest dimension is max_dim scale = max_dim / float(img.size[max_ax]) return img.resize((int(img.size[0] * scale), int(img.size[1] * scale))) def load_image_data(ids, max_dim=128, center=True): &quot;&quot;&quot; Takes as input an array of image ids and loads the images as numpy arrays with the images resized so the longest side is max-dim length. If center is True, then will place the image in the center of the output array, otherwise it will be placed at the top-left corner. &quot;&quot;&quot; # Initialize the output array # NOTE: Theano users comment line below and X = np.empty((len(ids), max_dim, max_dim, 1)) # X = np.empty((len(ids), 1, max_dim, max_dim)) # uncomment this for i, idee in enumerate(tqdm_notebook(list(ids))): # Turn the image into an array #x = resize_img(load_img(os.path.join(root, &#39;data/images&#39;, str(idee) + &#39;.jpg&#39;), grayscale=True), max_dim=max_dim) x = load_img(os.path.join(root, &#39;data/images&#39;, str(idee) + &#39;.jpg&#39;), grayscale=True) x = np.asarray(x, dtype=np.uint8) x = cv2.resize(x, (max_dim, max_dim), interpolation=cv2.INTER_LINEAR) #x = cv2.Canny(x,100,200) # Les 100 et 200 sont des seuils du filtre à régler x = img_to_array(x) # Get the corners of the bounding box for the image length = x.shape[0] width = x.shape[1] if center: h1 = int((max_dim - length) / 2) h2 = h1 + length w1 = int((max_dim - width) / 2) w2 = w1 + width else: h1, w1 = 0, 0 h2, w2 = (length, width) # Insert into image matrix X[i, h1:h2, w1:w2, 0:1] = x # Scale the array values so they are between 0 and 1 return np.around(X / 255.0) def load_train_data(split=split, random_state=None): &quot;&quot;&quot; Loads the pre-extracted feature and image training data and splits them into training and cross-validation. Returns one tuple for the training data and one for the validation data. Each tuple is in the order pre-extracted features, images, and labels. &quot;&quot;&quot; # Load the pre-extracted features ID, X_num_tr, y = load_numeric_training() # Load the image data X_img_tr = load_image_data(ID) # Split them into validation and cross-validation sss = StratifiedShuffleSplit(n_splits=1, train_size=split, random_state=random_state) train_ind, test_ind = next(sss.split(X_num_tr, y)) X_num_val, X_img_val, y_val = X_num_tr[test_ind], X_img_tr[test_ind], y[test_ind] X_num_tr, X_img_tr, y_tr = X_num_tr[train_ind], X_img_tr[train_ind], y[train_ind] return (X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val) def load_test_data(): &quot;&quot;&quot; Loads the pre-extracted feature and image test data. Returns a tuple in the order ids, pre-extracted features, and images. &quot;&quot;&quot; # Load the pre-extracted features ID, X_num_te = load_numeric_test() # Load the image data X_img_te = load_image_data(ID) return ID, X_num_te, X_img_te . print(&#39;Loading the training data...&#39;) (X_num_tr, X_img_tr, y_tr), (X_num_val, X_img_val, y_val) = load_train_data(random_state=split_random_state) y_tr_cat = to_categorical(y_tr) y_val_cat = to_categorical(y_val) print(&#39;Training data loaded!&#39;) . Loading the training data... . /tmp/ipykernel_27737/3251650489.py:54: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` for i, idee in enumerate(tqdm_notebook(list(ids))): /usr/local/lib/python3.8/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = &#34;grayscale&#34; warnings.warn(&#39;grayscale is deprecated. Please use &#39; . Training data loaded! . Les features num&#233;riques . train = pd.read_csv(os.path.join(root, &#39;data/train.csv&#39;)) train.pop(&#39;id&#39;) train.head() . species margin1 margin2 margin3 margin4 margin5 margin6 margin7 margin8 margin9 ... texture55 texture56 texture57 texture58 texture59 texture60 texture61 texture62 texture63 texture64 . 0 Acer_Opalus | 0.007812 | 0.023438 | 0.023438 | 0.003906 | 0.011719 | 0.009766 | 0.027344 | 0.0 | 0.001953 | ... | 0.007812 | 0.000000 | 0.002930 | 0.002930 | 0.035156 | 0.0 | 0.0 | 0.004883 | 0.000000 | 0.025391 | . 1 Pterocarya_Stenoptera | 0.005859 | 0.000000 | 0.031250 | 0.015625 | 0.025391 | 0.001953 | 0.019531 | 0.0 | 0.000000 | ... | 0.000977 | 0.000000 | 0.000000 | 0.000977 | 0.023438 | 0.0 | 0.0 | 0.000977 | 0.039062 | 0.022461 | . 2 Quercus_Hartwissiana | 0.005859 | 0.009766 | 0.019531 | 0.007812 | 0.003906 | 0.005859 | 0.068359 | 0.0 | 0.000000 | ... | 0.154300 | 0.000000 | 0.005859 | 0.000977 | 0.007812 | 0.0 | 0.0 | 0.000000 | 0.020508 | 0.002930 | . 3 Tilia_Tomentosa | 0.000000 | 0.003906 | 0.023438 | 0.005859 | 0.021484 | 0.019531 | 0.023438 | 0.0 | 0.013672 | ... | 0.000000 | 0.000977 | 0.000000 | 0.000000 | 0.020508 | 0.0 | 0.0 | 0.017578 | 0.000000 | 0.047852 | . 4 Quercus_Variabilis | 0.005859 | 0.003906 | 0.048828 | 0.009766 | 0.013672 | 0.015625 | 0.005859 | 0.0 | 0.000000 | ... | 0.096680 | 0.000000 | 0.021484 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | 0.000000 | 0.031250 | . 5 rows × 193 columns . train.describe() . margin1 margin2 margin3 margin4 margin5 margin6 margin7 margin8 margin9 margin10 ... texture55 texture56 texture57 texture58 texture59 texture60 texture61 texture62 texture63 texture64 . count 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | ... | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | 990.000000 | . mean 0.017412 | 0.028539 | 0.031988 | 0.023280 | 0.014264 | 0.038579 | 0.019202 | 0.001083 | 0.007167 | 0.018639 | ... | 0.036501 | 0.005024 | 0.015944 | 0.011586 | 0.016108 | 0.014017 | 0.002688 | 0.020291 | 0.008989 | 0.019420 | . std 0.019739 | 0.038855 | 0.025847 | 0.028411 | 0.018390 | 0.052030 | 0.017511 | 0.002743 | 0.008933 | 0.016071 | ... | 0.063403 | 0.019321 | 0.023214 | 0.025040 | 0.015335 | 0.060151 | 0.011415 | 0.039040 | 0.013791 | 0.022768 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 0.001953 | 0.001953 | 0.013672 | 0.005859 | 0.001953 | 0.000000 | 0.005859 | 0.000000 | 0.001953 | 0.005859 | ... | 0.000000 | 0.000000 | 0.000977 | 0.000000 | 0.004883 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000977 | . 50% 0.009766 | 0.011719 | 0.025391 | 0.013672 | 0.007812 | 0.015625 | 0.015625 | 0.000000 | 0.005859 | 0.015625 | ... | 0.004883 | 0.000000 | 0.005859 | 0.000977 | 0.012695 | 0.000000 | 0.000000 | 0.003906 | 0.002930 | 0.011719 | . 75% 0.025391 | 0.041016 | 0.044922 | 0.029297 | 0.017578 | 0.056153 | 0.029297 | 0.000000 | 0.007812 | 0.027344 | ... | 0.043701 | 0.000000 | 0.022217 | 0.009766 | 0.021484 | 0.000000 | 0.000000 | 0.023438 | 0.012695 | 0.029297 | . max 0.087891 | 0.205080 | 0.156250 | 0.169920 | 0.111330 | 0.310550 | 0.091797 | 0.031250 | 0.076172 | 0.097656 | ... | 0.429690 | 0.202150 | 0.172850 | 0.200200 | 0.106450 | 0.578130 | 0.151370 | 0.375980 | 0.086914 | 0.141600 | . 8 rows × 192 columns . Correlation map . corr = train.corr() # Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True . f, ax = plt.subplots(figsize=(11, 9)) cmap = sns.diverging_palette(220, 10, as_cmap=True) sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, square=True, xticklabels=5, yticklabels=5, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}, ax=ax) . &lt;AxesSubplot:&gt; . Entrainement rapide d&#39;une r&#233;gression logisitique . t = time.time() warnings.filterwarnings(&quot;ignore&quot;, category=FutureWarning) # Some iterations are not convering because of not really realistic combinations of penalty clf = LogisticRegression() clf.fit(X_num_tr, y_tr) print(&#39;Time elapsed :&#39;, round(time.time() - t,2), &#39;seconds&#39;) . Time elapsed : 2.62 seconds . /usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . y_pred_val = clf.predict(X_num_val) print(&#39;Loss cross entropy :&#39;, cross_entropy(y_pred_val, y_val)) print(&#39;Précision :&#39;, accuracy_score(y_val, y_pred_val)) . Loss cross entropy : -4.976226812466664e-08 Précision : 0.9932885906040269 . t = time.time() warnings.filterwarnings(&quot;ignore&quot;, category=sklearn.exceptions.ConvergenceWarning) # Some iterations are not convering because of not really realistic combinations of penalty clf = LogisticRegressionCV(Cs = np.logspace(-4, 4, num=10), cv=3, penalty = &#39;elasticnet&#39;, solver=&#39;saga&#39;, max_iter=200, l1_ratios=np.linspace(0, 1, num=10), n_jobs=30) clf.fit(X_num_tr, y_tr) print(&#39;Time elapsed :&#39;, round(time.time() - t,2), &#39;seconds&#39;) . Time elapsed : 820.03 seconds . y_pred_val = clf.predict(X_num_val) print(&#39;Loss cross entropy :&#39;, cross_entropy(y_pred_val, y_val)) print(&#39;Précision :&#39;, accuracy_score(y_val, y_pred_val)) . Loss cross entropy : -4.976226812466664e-08 Précision : 0.9932885906040269 . f = open(&#39;weights/RegLog.pckl&#39;, &#39;wb&#39;) pickle.dump(clf, f) f.close() . f = open(&#39;weights/RegLog.pckl&#39;, &#39;rb&#39;) clf = pickle.load(f) f.close() . C&#39;est bien et explicatif mais sans doute que l&#39;on peut faire mieux en utilisant aussi les images !! . Visualisation de quelques images . plt.figure(figsize=(30,15)) for i in range(4): plt.subplot(2,2,i+1) plt.imshow(X_img_tr[i].reshape((128,128)), &#39;gray&#39;) plt.title(&#39;Label de l &#39;image : &#39; + str(y_tr[i])) . print(&#39;Nombre d &#39;images d &#39;entrainement :% d.&#39; %(len(X_img_tr))) . Nombre d&#39;images d&#39;entrainement : 841. . Data Augmentation . On va utiliser un truc, classique avec les CNN, pour agrandir notre jeu de donnéres et améliorer la robustesse de notre classifieur. . On va faire des rotations, des retournements horizontaux et verticaux, des zooms aléatoires, etc... Avec des images de couleur on peut aussi jouer sur le contraste, correction gamma etc.. . NOTE: Ce n&#39;est pas non plus magique vu qu&#39;on ajoute aucune information extérieure. . from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator, array_to_img # Une toute petite modifs pour toujours avoir accès aux indices des images class ImageDataGenerator2(ImageDataGenerator): def flow(self, x, y=None, batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix=&#39;&#39;, save_format=&#39;jpeg&#39;): return NumpyArrayIterator2( x, y, self, batch_size=batch_size, shuffle=shuffle, seed=seed, save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format) class NumpyArrayIterator2(NumpyArrayIterator): def next(self): # for python 2.x. # Keeps under lock only the mechanism which advances # the indexing of each batch # see http://anandology.com/blog/using-iterators-and-generators/ with self.lock: # We changed index_array to self.index_array self.index_array, current_index, current_batch_size = next(self.index_generator) # The transformation of images is not under thread lock so it can be done in parallel batch_x = np.zeros(tuple([current_batch_size] + list(self.x.shape)[1:])) for i, j in enumerate(self.index_array): x = self.x[j] x = self.image_data_generator.random_transform(x.astype(&#39;float32&#39;)) x = self.image_data_generator.standardize(x) batch_x[i] = x if self.save_to_dir: for i in range(current_batch_size): img = array_to_img(batch_x[i], self.dim_ordering, scale=True) fname = &#39;{prefix}_{index}_{hash}.{format}&#39;.format(prefix=self.save_prefix, index=current_index + i, hash=np.random.randint(1e4), format=self.save_format) img.save(os.path.join(self.save_to_dir, fname)) if self.y is None: return batch_x batch_y = self.y[self.index_array] return batch_x, batch_y . print(&#39;Creating Data Augmenter...&#39;) imgen = ImageDataGenerator2( rotation_range=20, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True, vertical_flip=True, shear_range=0.2, fill_mode=&#39;nearest&#39;) imgen_train = imgen.flow(X_img_tr, y_tr_cat, seed=42) print(&#39;Finished making data augmenter...&#39;) . Creating Data Augmenter... Finished making data augmenter... . Combinaisons des features du CNN et des features extraites . Ca y est toutes nos données sont prêtes, on va pouvoir passer au modèle. . Keras Functional API . Il existe ausi une Keras&#39;s Sequential API, encore plus simple mais nous ne pouvons pas l&#39;utiliser ici à cause de cette combinaison entre réseau CNN et ajout des features pré-extraites. . from keras.models import Model from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten, Input, concatenate, LeakyReLU from keras.layers.normalization import BatchNormalization . ImportError Traceback (most recent call last) /tmp/ipykernel_27737/984225477.py in &lt;module&gt; 1 from keras.models import Model 2 from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten, Input, concatenate, LeakyReLU -&gt; 3 from keras.layers.normalization import BatchNormalization ImportError: cannot import name &#39;BatchNormalization&#39; from &#39;keras.layers.normalization&#39; (/usr/local/lib/python3.8/dist-packages/keras/layers/normalization/__init__.py) . def combined_model(): image = Input(shape=(128, 128, 1), name=&#39;image&#39;) x = Conv2D(8, (5, 5), input_shape=(128, 128, 1), padding=&#39;same&#39;)(image) x = (Activation(&#39;relu&#39;))(x) x = (Dropout(.2))(x) x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x) x = (Conv2D(32, (5, 5), padding=&#39;same&#39;))(x) x = (Activation(&#39;relu&#39;))(x) x = (Dropout(.2))(x) x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x) x = Flatten()(x) numerical = Input(shape=(192,), name=&#39;numerical&#39;) # Concatenate the output of our convnet with our pre-extracted feature input concatenated = concatenate([x, numerical]) x = Dense(100, activation=&#39;relu&#39;)(concatenated) x = Dropout(.2)(x) out = Dense(99, activation=&#39;softmax&#39;)(x) model = Model(inputs=[image, numerical], outputs=out) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) return model . model = combined_model() model.summary() . ____________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ==================================================================================================== image (InputLayer) (None, 128, 128, 1) 0 ____________________________________________________________________________________________________ conv2d_1 (Conv2D) (None, 128, 128, 8) 208 image[0][0] ____________________________________________________________________________________________________ activation_1 (Activation) (None, 128, 128, 8) 0 conv2d_1[0][0] ____________________________________________________________________________________________________ dropout_1 (Dropout) (None, 128, 128, 8) 0 activation_1[0][0] ____________________________________________________________________________________________________ max_pooling2d_1 (MaxPooling2D) (None, 64, 64, 8) 0 dropout_1[0][0] ____________________________________________________________________________________________________ conv2d_2 (Conv2D) (None, 64, 64, 32) 6432 max_pooling2d_1[0][0] ____________________________________________________________________________________________________ activation_2 (Activation) (None, 64, 64, 32) 0 conv2d_2[0][0] ____________________________________________________________________________________________________ dropout_2 (Dropout) (None, 64, 64, 32) 0 activation_2[0][0] ____________________________________________________________________________________________________ max_pooling2d_2 (MaxPooling2D) (None, 32, 32, 32) 0 dropout_2[0][0] ____________________________________________________________________________________________________ flatten_1 (Flatten) (None, 32768) 0 max_pooling2d_2[0][0] ____________________________________________________________________________________________________ numerical (InputLayer) (None, 192) 0 ____________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 32960) 0 flatten_1[0][0] numerical[0][0] ____________________________________________________________________________________________________ dense_1 (Dense) (None, 100) 3296100 concatenate_1[0][0] ____________________________________________________________________________________________________ dropout_3 (Dropout) (None, 100) 0 dense_1[0][0] ____________________________________________________________________________________________________ dense_2 (Dense) (None, 99) 9999 dropout_3[0][0] ==================================================================================================== Total params: 3,312,739 Trainable params: 3,312,739 Non-trainable params: 0 ____________________________________________________________________________________________________ . Pour que ça tourne vite il faut faire tourner sur GPU ! . from keras.utils import plot_model plot_model(model, to_file=&#39;files/model_plot.png&#39;, show_shapes=True, show_layer_names=True) . . from keras.callbacks import ModelCheckpoint, TensorBoard from keras.models import load_model def combined_generator(imgen, X): &quot;&quot;&quot; A generator to train our keras neural network. It takes the image augmenter generator and the array of the pre-extracted features. It yields a minibatch and will run indefinitely &quot;&quot;&quot; while True: for i in range(X.shape[0]): # Get the image batch and labels batch_img, batch_y = next(imgen) # We can now access the indicies # of the images that imgen gave us. x = X[imgen.index_array] yield [batch_img, x], batch_y batch_size = 128 epochs = 50 . %%time best_model_file = &quot;weights/leafnet.h5&quot; best_model = ModelCheckpoint(best_model_file, monitor=&#39;val_loss&#39;, verbose=0, save_best_only=True) tbCallBack = TensorBoard(log_dir=&#39;./graph&#39;, histogram_freq=1, write_graph=True, write_images=True) history = model.fit_generator(combined_generator(imgen_train, X_num_tr), steps_per_epoch=np.ceil(X_num_tr.shape[0] / batch_size), epochs=epochs, validation_data=([X_img_val, X_num_val], y_val_cat), verbose=1, callbacks=[best_model,tbCallBack ]) . Epoch 1/50 7/7 [==============================] - 3s - loss: 4.7922 - acc: 0.0134 - val_loss: 4.5294 - val_acc: 0.0604 Epoch 2/50 7/7 [==============================] - 2s - loss: 4.4993 - acc: 0.0714 - val_loss: 4.3921 - val_acc: 0.1544 Epoch 3/50 7/7 [==============================] - 2s - loss: 4.3459 - acc: 0.1071 - val_loss: 4.1990 - val_acc: 0.1678 Epoch 4/50 7/7 [==============================] - 2s - loss: 4.2154 - acc: 0.1096 - val_loss: 3.9726 - val_acc: 0.2685 Epoch 5/50 7/7 [==============================] - 2s - loss: 3.8082 - acc: 0.2679 - val_loss: 3.6570 - val_acc: 0.2953 Epoch 6/50 7/7 [==============================] - 2s - loss: 3.6255 - acc: 0.2054 - val_loss: 3.3958 - val_acc: 0.3826 Epoch 7/50 7/7 [==============================] - 2s - loss: 3.2309 - acc: 0.3348 - val_loss: 3.0281 - val_acc: 0.4161 Epoch 8/50 7/7 [==============================] - 2s - loss: 3.0134 - acc: 0.3524 - val_loss: 2.7002 - val_acc: 0.5906 Epoch 9/50 7/7 [==============================] - 2s - loss: 2.6414 - acc: 0.4464 - val_loss: 2.5139 - val_acc: 0.6510 Epoch 10/50 7/7 [==============================] - 2s - loss: 2.3679 - acc: 0.4911 - val_loss: 2.1298 - val_acc: 0.7114 Epoch 11/50 7/7 [==============================] - 2s - loss: 2.0286 - acc: 0.6250 - val_loss: 1.8708 - val_acc: 0.7852 Epoch 12/50 7/7 [==============================] - 2s - loss: 1.7696 - acc: 0.6803 - val_loss: 1.6398 - val_acc: 0.8322 Epoch 13/50 7/7 [==============================] - 2s - loss: 1.5771 - acc: 0.7009 - val_loss: 1.4812 - val_acc: 0.8523 Epoch 14/50 7/7 [==============================] - 2s - loss: 1.4238 - acc: 0.7768 - val_loss: 1.2679 - val_acc: 0.8389 Epoch 15/50 7/7 [==============================] - 2s - loss: 1.2462 - acc: 0.7902 - val_loss: 1.2103 - val_acc: 0.8792 Epoch 16/50 7/7 [==============================] - 2s - loss: 1.2004 - acc: 0.7129 - val_loss: 0.9805 - val_acc: 0.9060 Epoch 17/50 7/7 [==============================] - 2s - loss: 1.0450 - acc: 0.8080 - val_loss: 0.9416 - val_acc: 0.9396 Epoch 18/50 7/7 [==============================] - 2s - loss: 0.9344 - acc: 0.8661 - val_loss: 0.8538 - val_acc: 0.9463 Epoch 19/50 7/7 [==============================] - 2s - loss: 0.8161 - acc: 0.8482 - val_loss: 0.7642 - val_acc: 0.9530 Epoch 20/50 7/7 [==============================] - 2s - loss: 0.7789 - acc: 0.8642 - val_loss: 0.6700 - val_acc: 0.9396 Epoch 21/50 7/7 [==============================] - 2s - loss: 0.6245 - acc: 0.8839 - val_loss: 0.6394 - val_acc: 0.9262 Epoch 22/50 7/7 [==============================] - 2s - loss: 0.6922 - acc: 0.8571 - val_loss: 0.5840 - val_acc: 0.9463 Epoch 23/50 7/7 [==============================] - 2s - loss: 0.6807 - acc: 0.8705 - val_loss: 0.5557 - val_acc: 0.9530 Epoch 24/50 7/7 [==============================] - 2s - loss: 0.5348 - acc: 0.9130 - val_loss: 0.5055 - val_acc: 0.9396 Epoch 25/50 7/7 [==============================] - 2s - loss: 0.4969 - acc: 0.9241 - val_loss: 0.5246 - val_acc: 0.9530 Epoch 26/50 7/7 [==============================] - 2s - loss: 0.5345 - acc: 0.9107 - val_loss: 0.4434 - val_acc: 0.9597 Epoch 27/50 7/7 [==============================] - 2s - loss: 0.6305 - acc: 0.8242 - val_loss: 0.4436 - val_acc: 0.9664 Epoch 28/50 7/7 [==============================] - 2s - loss: 0.4488 - acc: 0.9018 - val_loss: 0.3930 - val_acc: 0.9664 Epoch 29/50 7/7 [==============================] - 2s - loss: 0.3734 - acc: 0.9464 - val_loss: 0.4096 - val_acc: 0.9396 Epoch 30/50 7/7 [==============================] - 2s - loss: 0.4251 - acc: 0.9286 - val_loss: 0.3380 - val_acc: 0.9866 Epoch 31/50 7/7 [==============================] - 2s - loss: 0.3852 - acc: 0.9176 - val_loss: 0.3643 - val_acc: 0.9664 Epoch 32/50 7/7 [==============================] - 2s - loss: 0.3393 - acc: 0.9464 - val_loss: 0.3464 - val_acc: 0.9664 Epoch 33/50 7/7 [==============================] - 2s - loss: 0.3156 - acc: 0.9464 - val_loss: 0.3179 - val_acc: 0.9799 Epoch 34/50 7/7 [==============================] - 2s - loss: 0.3403 - acc: 0.9509 - val_loss: 0.3087 - val_acc: 0.9664 Epoch 35/50 7/7 [==============================] - 2s - loss: 0.2998 - acc: 0.9312 - val_loss: 0.3232 - val_acc: 0.9597 Epoch 36/50 7/7 [==============================] - 2s - loss: 0.2712 - acc: 0.9598 - val_loss: 0.2596 - val_acc: 0.9732 Epoch 37/50 7/7 [==============================] - 2s - loss: 0.3331 - acc: 0.9241 - val_loss: 0.3021 - val_acc: 0.9597 Epoch 38/50 7/7 [==============================] - 2s - loss: 0.2584 - acc: 0.9688 - val_loss: 0.2465 - val_acc: 0.9732 Epoch 39/50 7/7 [==============================] - 2s - loss: 0.2976 - acc: 0.9547 - val_loss: 0.2265 - val_acc: 0.9732 Epoch 40/50 7/7 [==============================] - 2s - loss: 0.2624 - acc: 0.9598 - val_loss: 0.2696 - val_acc: 0.9597 Epoch 41/50 7/7 [==============================] - 2s - loss: 0.2349 - acc: 0.9598 - val_loss: 0.2323 - val_acc: 0.9664 Epoch 42/50 7/7 [==============================] - 2s - loss: 0.2439 - acc: 0.9688 - val_loss: 0.2264 - val_acc: 0.9732 Epoch 43/50 7/7 [==============================] - 2s - loss: 0.2475 - acc: 0.9719 - val_loss: 0.2236 - val_acc: 0.9732 Epoch 44/50 7/7 [==============================] - 2s - loss: 0.2160 - acc: 0.9732 - val_loss: 0.2230 - val_acc: 0.9732 Epoch 45/50 7/7 [==============================] - 2s - loss: 0.1916 - acc: 0.9821 - val_loss: 0.2235 - val_acc: 0.9664 Epoch 46/50 7/7 [==============================] - 2s - loss: 0.2529 - acc: 0.9554 - val_loss: 0.2196 - val_acc: 0.9597 Epoch 47/50 7/7 [==============================] - 2s - loss: 0.1934 - acc: 0.9728 - val_loss: 0.1741 - val_acc: 0.9866 Epoch 48/50 7/7 [==============================] - 2s - loss: 0.1795 - acc: 0.9821 - val_loss: 0.1778 - val_acc: 0.9933 Epoch 49/50 7/7 [==============================] - 2s - loss: 0.1751 - acc: 0.9732 - val_loss: 0.1955 - val_acc: 0.9732 Epoch 50/50 7/7 [==============================] - 2s - loss: 0.1795 - acc: 0.9688 - val_loss: 0.1573 - val_acc: 0.9933 CPU times: user 7min 16s, sys: 10.8 s, total: 7min 27s Wall time: 1min 55s . y_pred_val = model.predict([X_img_val, X_num_val]) y_pred_val = np.argmax(y_pred_val,axis=1) print(&#39;Loss cross entropy :&#39;, cross_entropy(y_pred_val, y_val)) print(&#39;Précision :&#39;, accuracy_score(y_val, y_pred_val)) . Loss cross entropy : -4.976226812466664e-08 Précision : 0.9932885906040269 . Visualisation . Bon on peut quand même se demander ce qui a été appris par le CNN et que ces features ont un sens ! On va donc visualiser quelques couches cachées. On va donc prendre quelques images du set de validation, et on va les faire passer à travers le réseau pour visualiser les résultats de différents filtrages à l&#39;intérieur du CNN pour voir quelles parties sont activées. Dans notre architecture, il y a 8 filtres pour la première couche et 32 pour la seconde donc 40 images filtrées au total ! . import pandas as pd LABELS = sorted(pd.read_csv(os.path.join(root, &#39;data/train.csv&#39;)).species.unique()) . from math import sqrt import matplotlib.pyplot as plt from keras import backend as K NUM_LEAVES = 3 model_fn = &#39;weights/leafnet.h5&#39; # Function by gcalmettes from http://stackoverflow.com/questions/11159436/multiple-figures-in-a-single-window def plot_figures(figures, nrows = 1, ncols=1, titles=False): &quot;&quot;&quot;Plot a dictionary of figures. Parameters - figures : &lt;title, figure&gt; dictionary ncols : number of columns of subplots wanted in the display nrows : number of rows of subplots wanted in the figure &quot;&quot;&quot; plt.figure(figsize=(30,15)) fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows) for ind,title in enumerate(sorted(figures.keys(), key=lambda s: int(s[3:]))): axeslist.ravel()[ind].imshow(figures[title], cmap=plt.gray()) if titles: axeslist.ravel()[ind].set_title(title) for ind in range(nrows*ncols): axeslist.ravel()[ind].set_axis_off() if titles: plt.tight_layout() plt.show() def get_dim(num): &quot;&quot;&quot; Simple function to get the dimensions of a square-ish shape for plotting num images &quot;&quot;&quot; s = sqrt(num) if round(s) &lt; s: return (int(s), int(s)+1) else: return (int(s)+1, int(s)+1) . model = load_model(model_fn) # Get the convolutional layers conv_layers = [layer for layer in model.layers if isinstance(layer, MaxPooling2D)] # Pick random images to visualize imgs_to_visualize = np.random.choice(np.arange(0, len(X_img_val)), NUM_LEAVES) # Use a keras function to extract the conv layer data convout_func = K.function([model.layers[0].input, K.learning_phase()], [layer.output for layer in conv_layers]) conv_imgs_filts = convout_func([X_img_val[imgs_to_visualize], 0]) # Also get the prediction so we know what we predicted predictions = model.predict([X_img_val[imgs_to_visualize], X_num_val[imgs_to_visualize]]) imshow = plt.imshow #alias # Loop through each image disply relevant info . def show_filtered(imgs_to_visualize, img_count): img_to_visualize = imgs_to_visualize[img_count] top3_ind = predictions[img_count].argsort()[-3:] top3_species = np.array(LABELS)[top3_ind] top3_preds = predictions[img_count][top3_ind] actual = LABELS[y_val[img_to_visualize]] print(&quot;Top 3 Predicitons:&quot;) for i in range(2, -1, -1): print(&quot; t%s: %s&quot; % (top3_species[i], top3_preds[i])) print(&quot; nActual: %s&quot; % actual) for i, conv_imgs_filt in enumerate(conv_imgs_filts): conv_img_filt = conv_imgs_filt[img_count] print(&quot;Visualizing Convolutions Layer %d&quot; % i) fig_dict = {&#39;flt{0}&#39;.format(i): conv_img_filt[:, :, i] for i in range(conv_img_filt.shape[-1])} plot_figures(fig_dict, *get_dim(len(fig_dict))) plt.figure(figsize=(15,7)) plt.title(&quot;Image used: #%d (digit=%d)&quot; % (img_to_visualize, y_val[img_to_visualize])) imshow(X_img_val[img_to_visualize][:, :, 0], cmap=&#39;gray&#39;) plt.tight_layout() . show_filtered(imgs_to_visualize, 0) . Top 3 Predicitons: Quercus_Hartwissiana: 0.7982975 Quercus_Canariensis: 0.046233166 Celtis_Koraiensis: 0.034486573 Actual: Quercus_Hartwissiana Visualizing Convolutions Layer 0 . &lt;Figure size 2160x1080 with 0 Axes&gt; . Visualizing Convolutions Layer 1 . &lt;Figure size 2160x1080 with 0 Axes&gt; . show_filtered(imgs_to_visualize, 1) . Top 3 Predicitons: Fagus_Sylvatica: 0.48016998 Cercis_Siliquastrum: 0.32172352 Quercus_Infectoria_sub: 0.064567514 Actual: Fagus_Sylvatica Visualizing Convolutions Layer 0 . &lt;Figure size 2160x1080 with 0 Axes&gt; . Visualizing Convolutions Layer 1 . &lt;Figure size 2160x1080 with 0 Axes&gt; . show_filtered(imgs_to_visualize, 2) . Top 3 Predicitons: Quercus_Rhysophylla: 0.97699624 Cornus_Chinensis: 0.006621217 Quercus_Ilex: 0.0035838485 Actual: Quercus_Rhysophylla Visualizing Convolutions Layer 0 . &lt;Figure size 2160x1080 with 0 Axes&gt; . Visualizing Convolutions Layer 1 . &lt;Figure size 2160x1080 with 0 Axes&gt; . for img_count, img_to_visualize in enumerate(imgs_to_visualize): top3_ind = predictions[img_count].argsort()[-3:] top3_species = np.array(LABELS)[top3_ind] top3_preds = predictions[img_count][top3_ind] actual = LABELS[y_val[img_to_visualize]] print(&quot;Top 3 Predicitons:&quot;) for i in range(2, -1, -1): print(&quot; t%s: %s&quot; % (top3_species[i], top3_preds[i])) print(&quot; nActual: %s&quot; % actual) plt.figure(figsize=(15,7)) plt.title(&quot;Image used: #%d (digit=%d)&quot; % (img_to_visualize, y_val[img_to_visualize])) imshow(X_img_val[img_to_visualize][:, :, 0], cmap=&#39;gray&#39;) plt.tight_layout() for i, conv_imgs_filt in enumerate(conv_imgs_filts): conv_img_filt = conv_imgs_filt[img_count] print(&quot;Visualizing Convolutions Layer %d&quot; % i) fig_dict = {&#39;flt{0}&#39;.format(i): conv_img_filt[:, :, i] for i in range(conv_img_filt.shape[-1])} plot_figures(fig_dict, *get_dim(len(fig_dict))) . Top 3 Predicitons: Quercus_Hartwissiana: 0.7982975 Quercus_Canariensis: 0.046233166 Celtis_Koraiensis: 0.034486573 Actual: Quercus_Hartwissiana Visualizing Convolutions Layer 0 . &lt;Figure size 2160x1080 with 0 Axes&gt; . Visualizing Convolutions Layer 1 . &lt;Figure size 2160x1080 with 0 Axes&gt; . Top 3 Predicitons: Fagus_Sylvatica: 0.48016998 Cercis_Siliquastrum: 0.32172352 Quercus_Infectoria_sub: 0.064567514 Actual: Fagus_Sylvatica Visualizing Convolutions Layer 0 . &lt;Figure size 2160x1080 with 0 Axes&gt; . Visualizing Convolutions Layer 1 . &lt;Figure size 2160x1080 with 0 Axes&gt; . Top 3 Predicitons: Quercus_Rhysophylla: 0.97699624 Cornus_Chinensis: 0.006621217 Quercus_Ilex: 0.0035838485 Actual: Quercus_Rhysophylla Visualizing Convolutions Layer 0 . &lt;Figure size 2160x1080 with 0 Axes&gt; . Visualizing Convolutions Layer 1 . &lt;Figure size 2160x1080 with 0 Axes&gt; . del model . API avec Flask . Web micro-framework en Python (utilisé par Pinterest et Linkedin d&#39;après Wikipedia). . . Léger | Rapide | Facile d&#39;utilisation | . Ici on va faire seulement le back-end ! Du dev supplémentaire serait nécessaire pour rajouter un front-end. . !cd api &amp;&amp; nohup python3 deploy.py . nohup: les entrées sont ignorées et la sortie est ajoutée à &#39;nohup.out&#39; ^C . %%time import requests import json headers = {&quot;Content-Type&quot;: &quot;application/json&quot;} payload = [X_img_val[93:94].tolist(), X_num_val[93:94].tolist()] r = requests.post(&quot;http://0.0.0.0:5000/predict&quot;, json=payload, headers=headers) response = json.loads(r.text) print(response[&#39;success&#39;]) . True CPU times: user 44 ms, sys: 8 ms, total: 52 ms Wall time: 84 ms . response[&#39;prediction&#39;] = response[&#39;prediction&#39;].replace(&#39;[[&#39;,&#39;&#39;) response[&#39;prediction&#39;] = response[&#39;prediction&#39;].replace(&#39;]]&#39;,&#39;&#39;) predictions = np.array(response[&#39;prediction&#39;].split(),dtype=float) top3_ind = predictions.argsort()[-3:] top3_species = np.array(LABELS)[top3_ind] top3_preds = predictions[top3_ind] # Display the top 3 predictions and the actual species print(&quot;Top 3 Predicitons:&quot;) for i in range(2, -1, -1): print(&quot; t%s: %s&quot; % (top3_species[i], top3_preds[i])) . Top 3 Predicitons: Tilia_Platyphyllos: 0.81717563 Celtis_Koraiensis: 0.12848337 Pterocarya_Stenoptera: 0.010453341 .",
            "url": "https://ludovicdmt.github.io/ludodarmet/2019/06/06/keras-CNN.html",
            "relUrl": "/2019/06/06/keras-CNN.html",
            "date": " • Jun 6, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "💁‍♂️ About Me",
          "content": "I was born in 1993 in Annecy, in the French Alps. After high school and two years of preparatory classes for the Grandes Ecoles, I joined Ecole Centrale de Lille, a generalist engineering school in Lille, north of France. I graduated in 2017 with a specialization in Data Science and AI. I pursued with the GIPSA-lab for 3 years Ph.D., obtained in December 2020. I was supervised by Kai Wang and François Cayre. I worked on image manipulation detection, namely image forensics, funded by an ANR-DGA project managed by Patrick Bas. Since March 2021, I am part of the neuro-ergonomics and human-factors department from ISAE-Supaero. I work in the team of Frédéric Dehais on reactive BCI using Visually Evoqued Potentials and passive BCI. . Skills . &amp;nbsp Machine Learning and Artificial Intelligence | &amp;nbsp Python, MATLAB, GitHub, Pytorch, Tensorflow | &amp;nbspScientific writing &amp; editing | &amp;nbsp Teaching &amp; Dissemination | &amp;nbsp Project management | .",
          "url": "https://ludovicdmt.github.io/ludodarmet/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
      ,"page7": {
          "title": "🧑‍🏫 Teaching",
          "content": "I have taught the following courses: . PHELMA, Grenoble INP, 2018: Probabilities and Statistics (9*3h), Lecture and practical sessions, 1st-year student of engineering school (the equivalent of last year bachelor) . | PHELMA, Grenoble INP, 2019: Advanced computer vision (2*4h), Practical sessions, 2nd-year student of engineering school (the equivalent of first-year master) . | Grenoble University, 2019: Introduction to Python and Algorithms (9*3h), Practical sessions + end year project, First year of bachelor . | ENSE3, Grenoble INP, 2020: Signal Processing (4*4h), Practical sessions, Last year student of engineering school (the equivalent of second-year master) . | Grenoble University, 2020: Introduction to Python and Algorithms (9*3h), Practical sessions + end year project, First year of bachelor . | ISAE-Supaero, 2022: Machine Learning for EEG classification (4h), Lecture and practical session, Last year student of engineering school (the equivalent of second-year master). . | . I also have supervised interns and students projects: . Eloi Martinet (3 months, second-year engineer student in ENSIMAG, Grenoble INP), 2018: White box attacks and defense on the SpliceBuster classifier. . | Ambarish Parthasarathy (6 months, bachelor thesis in ENSIMAG, Grenoble INP), 2019: Copy-move image forgery detection. . | Marine de Fontenay (6 months, last year engineer student in ISAE-Supaero) et Lison Kardassevitch (3 months, last year bachelor student in applied mathematics in Toulouse University), 2021: Auditory attention decoding using EEG. . | Alice Déchelette, Guilhem Glaziou, Kinza Kasmi, and David Valdivia (last year project of engineering school, ISAE-Supaero), 2021: Students have collaborated with the Bee Ready company to develop an automatic tool for the analysis of video interviews. I was a scientific advisor for the project. . | .",
          "url": "https://ludovicdmt.github.io/ludodarmet/teaching/",
          "relUrl": "/teaching/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ludovicdmt.github.io/ludodarmet/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}